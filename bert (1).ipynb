{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   speaker                                               text\n",
      "0           0    Monica  There's nothing to tell! He's just some guy I ...\n",
      "1           1      Joey  C'mon, you're going out with the guy! There's ...\n",
      "2           2  Chandler  All right Joey, be nice. So does he have a hum...\n",
      "3           3    Phoebe                          Wait, does he eat chalk? \n",
      "4           4    Phoebe  Just, 'cause, I don't want her to go through w...\n",
      "32485\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/shellyschwartz/Downloads/friends_lines.csv\")\n",
    "print(df.head())\n",
    "print(len(df))\n",
    "possible_labels = df.speaker.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict\n",
    "df['label'] = df.speaker.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                             \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', num_labels=len(label_dict), return_dict=False)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 75,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 75,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 75,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,6)\n",
    "\n",
    "      #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-6)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.00957235 1.03084539 0.99693101 1.16180258 0.93971303 0.8997341 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "    # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        inputs = {'sent_id':      batch[0],\n",
    "                  'mask': batch[1],\n",
    "                 } \n",
    "        labels = batch[2]\n",
    "        #batch = [t.to(device) for t in batch]\n",
    "        \n",
    "        #sent_id, mask, labels = batch\n",
    "    # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "        preds = model(**inputs)\n",
    "        #preds = model(sent_id, mask)\n",
    "        \n",
    "    # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "        #total_preds.append(preds)\n",
    "        total_preds = np.append(total_preds, preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "        loss = cross_entropy(preds,labels)\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        preds = preds.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.795\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 2 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.793\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 3 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.792\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 4 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.792\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 5 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.792\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 6 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 7 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 8 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.790\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 9 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 10 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 11 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.790\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 12 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 13 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.790\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 14 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.790\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 15 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 16 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 17 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 18 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 19 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 20 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 21 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 22 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 23 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 24 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 25 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 26 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 27 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 28 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 29 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 30 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 31 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 32 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 33 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.787\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 34 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 35 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 36 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 37 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 38 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 39 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 40 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 41 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 42 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 43 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 44 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 45 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 46 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 47 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.784\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 48 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.784\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 49 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 0.005\n",
      "\n",
      " Epoch 50 / 50\n",
      "  Batch    50  of  1,523.\n",
      "  Batch   100  of  1,523.\n",
      "  Batch   150  of  1,523.\n",
      "  Batch   200  of  1,523.\n",
      "  Batch   250  of  1,523.\n",
      "  Batch   300  of  1,523.\n",
      "  Batch   350  of  1,523.\n",
      "  Batch   400  of  1,523.\n",
      "  Batch   450  of  1,523.\n",
      "  Batch   500  of  1,523.\n",
      "  Batch   550  of  1,523.\n",
      "  Batch   600  of  1,523.\n",
      "  Batch   650  of  1,523.\n",
      "  Batch   700  of  1,523.\n",
      "  Batch   750  of  1,523.\n",
      "  Batch   800  of  1,523.\n",
      "  Batch   850  of  1,523.\n",
      "  Batch   900  of  1,523.\n",
      "  Batch   950  of  1,523.\n",
      "  Batch 1,000  of  1,523.\n",
      "  Batch 1,050  of  1,523.\n",
      "  Batch 1,100  of  1,523.\n",
      "  Batch 1,150  of  1,523.\n",
      "  Batch 1,200  of  1,523.\n",
      "  Batch 1,250  of  1,523.\n",
      "  Batch 1,300  of  1,523.\n",
      "  Batch 1,350  of  1,523.\n",
      "  Batch 1,400  of  1,523.\n",
      "  Batch 1,450  of  1,523.\n",
      "  Batch 1,500  of  1,523.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    381.\n",
      "  Batch   100  of    381.\n",
      "  Batch   150  of    381.\n",
      "  Batch   200  of    381.\n",
      "  Batch   250  of    381.\n",
      "  Batch   300  of    381.\n",
      "  Batch   350  of    381.\n",
      "\n",
      "Training Loss: 1.784\n",
      "Validation Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.43      0.27       335\n",
      "           1       0.20      0.09      0.12       328\n",
      "           2       0.22      0.13      0.16       340\n",
      "           3       0.18      0.21      0.19       291\n",
      "           4       0.22      0.24      0.23       360\n",
      "           5       0.24      0.14      0.17       377\n",
      "\n",
      "    accuracy                           0.20      2031\n",
      "   macro avg       0.21      0.21      0.19      2031\n",
      "weighted avg       0.21      0.20      0.19      2031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
